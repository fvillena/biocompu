{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase pr치ctica 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clase Pr치ctica 1: Clasificaci칩n 游늳\n",
        "\n",
        "\n",
        "----------------------------------"
      ],
      "metadata": {
        "id": "6GAvF9Liisd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el aprendizaje de m치quinas, la tarea de clasificaci칩n consiste en predecir una variable discreta (perteneciente a un conjunto finito de opciones) a partir de una observaci칩n, es decir, un conjunto de caracter칤sticas. \n",
        "\n",
        "La principal diferencia entre problemas de clasificaci칩n y regresi칩n, es que en estos 칰ltimos los modelos predicen una variable continua (conjunto infinito de posibilidades) a partir de los datos entrada. As칤, un ejemplo de un problema de regresi칩n ser칤a predecir el precio de una casa a partir de las caracter칤sticas como el tama침o, ubicaci칩n, material, etc. De lo contrario, un ejemplo de clasificaci칩n ser칤a predecir si el contenido de un correo corresponde a un spam o no. En este caso, como el n칰mero de respuestas posibles es igual a 2 (finito), se dice que es un problema de clasificaci칩n binaria. \n",
        "\n",
        "## Objetivos de la clase 游닄\n",
        "\n",
        "El objetivo principal de esta primera clase pr치ctica es realizar una introducci칩n a la tarea de clasificaci칩n en el aprendizaje de m치quinas. Para esto, primero implementaremos varios modelos de clasificaci칩n para poder relacionar las caracter칤sticas fisiol칩gicas de pacientes con la presencia de una enfermedad cardiovascular 游뽘. \n",
        "\n",
        "Las principales librer칤as que vamos a utilizar son las siguientes:\n",
        "\n",
        "- Pandas 游냪: Manejo y an치lisis de estructuras de datos.\n",
        "- Scikit-learn: Librer칤a para el aprendizaje de m치quina, contiene no s칩lo modelos cl치sicos sino que tambi칠n una serie de funciones que nos permitir치n procesar mejor los datos de entrada.\n",
        "- Numpy: Manejo de operaciones matem치ticas.\n",
        "- Matplotlib y Seaborn: Generaci칩n de visualizaciones sobre los datos.\n",
        "\n",
        "## Metodolog칤a\n",
        "\n",
        "La mayor칤a de los problemas que se resuelven con aprendizaje de m치quinas pueden ser abordados siguiendo 3 pasos principales:\n",
        "\n",
        "1. Exploraci칩n de datos.\n",
        "2. Selecci칩n de modelos.\n",
        "3. Comparaci칩n y an치lisis de resultados.\n",
        "\n",
        "A pesar de que en aplicaciones reales existen muchos m치s pasos intermedios, como el estudio del problema a nivel de negocio y la puesta en producci칩n, estos pasos ser치n suficientes para abordar los distintos problemas que veremos en este curso.\n",
        "\n",
        "As칤 que empecemos! 游봅\n"
      ],
      "metadata": {
        "id": "ka41Y9Ciisyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Clasificaci칩n**"
      ],
      "metadata": {
        "id": "ZqfjgV1Deuyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploraci칩n de datos 游븷**"
      ],
      "metadata": {
        "id": "KRLw4Z42m2kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la mayor칤a de los casos, cuando se resuelven problemas con aprendizaje de m치quinas solemos trabajar con datos almacenados en formatos de tablas, con extensiones tales como *csv*, *tsv*, *xlsx* o incluso en algunos casos *txt* (m치s com칰n al trabajar con texto). \n",
        "\n",
        "*Pandas* 游냪 es una librer칤a de Python especializada en el manejo de estructuras de datos, la cual nos permite poder cargar, procesar y analizar datos tabulados de manera sencilla. Adem치s, nos entrega muchas funciones que en conjunto con otros librer칤as de visualizaci칩n como *Matplotlib* o *Seaborn* nos permite obtener excelentes visualizaciones sobre los datos."
      ],
      "metadata": {
        "id": "Jx90SmY_n5YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importar las librer칤as**"
      ],
      "metadata": {
        "id": "fO3ZlyjwmyBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns "
      ],
      "metadata": {
        "id": "LwdZuCQ6aEjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lectura del dataset**"
      ],
      "metadata": {
        "id": "uDcwDmSTm2ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la funci칩n `read_csv` podemos cargar datasets que se encuentren en formato csv. Es importante que el nombre de la variable sea lo m치s simple posible, ya que ser치 utilizado frecuentemente."
      ],
      "metadata": {
        "id": "JEbr1fQ-nZyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/fvillena/biocompu/2022/data/cardiovascular_diseases.csv\")"
      ],
      "metadata": {
        "id": "dOMFr3b-n6Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funci칩n `head` nos muestra las primeras 5 filas del dataset"
      ],
      "metadata": {
        "id": "lxV1Y0JzpBW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Yy8db6QLo_ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funci칩n `sample(x)` nos entrega x ejemplos del datasets al azar"
      ],
      "metadata": {
        "id": "CIXoza3Mp5Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(3)"
      ],
      "metadata": {
        "id": "G9z2xYlWpkfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funci칩n `tail` nos muestra las 칰ltimas filas del dataset"
      ],
      "metadata": {
        "id": "2TG4tFHKp-FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "c37lrSoSqBNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En aprendizaje de m치quinas las filas de un dataset son m치s conocidas como *instancias*, mientras que las columnas son nuestros *atributos* o *features*. Para acceder al n칰mero de instancias y de features utilizamos el atributo `shape`. El primer valor corresponde al n칰mero de instancia, mientras que el segundo es el n칰mero de atributos."
      ],
      "metadata": {
        "id": "S5T3OHVWqHUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape "
      ],
      "metadata": {
        "id": "Ro-c8oBjq0Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado de la celda anterior nos muestra que hay un total de 301 instancias, donde cada una tiene un total de 14 atributos. Para acceder al nombre de cada uno de estos atributos, utilizamos el comando `columns`."
      ],
      "metadata": {
        "id": "aaEGqmEFq_EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "dYe-2dTBrHNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seg칰n el resultado anterior, tenemos los siguientes datos del paciente: edad, sexo, tipo de dolor de pecho, colesterol, nivel alto de azucar en sangre en ayudo, resultados electrocardiogr치ficos en reposo, frecuencia cardiaca m치xica alcanzada, angina inducida por el ejercicio, depresi칩n st por ejercicio, entre otras."
      ],
      "metadata": {
        "id": "XeYWJrIiDNc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos informaci칩n m치s detallada acerca de estos atributos podemos utilizar la funci칩n `info`, la cual nos entrega el nombre, la cantidad de ejemplos no nulos y el tipo de dato de cada columna."
      ],
      "metadata": {
        "id": "gbZKXAW-rSO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "N3DOn9pYrW9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si queremos obtener estad칤stica acerca de cada una de nuestras columnas utilizamos la funci칩n `describe`. Esta funci칩n nos entrega la cantidad de datos, el promedio, desviaci칩n estandar, m칤nimo, m치ximo y cuartiles."
      ],
      "metadata": {
        "id": "Ok8HMFqPsY73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "UMWeVjRcsYVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un paso fundamental en la exploraci칩n de datos es ver que no hayan valores nulos. Para revisar esto de manera r치pida, utilizamos la funci칩n `isnull`. Esta funci칩n analiza celda por celda si existe un valor nulo, en caso que as칤 sea asigna el valor `True`, en caso contrario se coloca el valor `False`."
      ],
      "metadata": {
        "id": "hvPOEUsbstVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull()"
      ],
      "metadata": {
        "id": "_-74WtvwsqEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, podemos utilizar la funci칩n `sum`, que nos permite sumar los valores que son `True` en cada columna."
      ],
      "metadata": {
        "id": "pu1b9n7mR5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "2JyegUTYs17R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso no tenemos valores nulos, pero si los tuvi칠ramos es importante que sean reemplazados por alg칰n valor. \n",
        "\n",
        "Ahora vamos a analizar nuestro **atributo objetivo**. Este atributo corresponde al valor que nos gustar칤a predecir a partir de los dem치s atributos (o parte de ellos). En nuestro caso utilizaremos la columna  *cardiovascular_disease*. Accedamos a ella para ver la naturaleza de los datos."
      ],
      "metadata": {
        "id": "6haIf2cDto4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"cardiovascular_disease\"]"
      ],
      "metadata": {
        "id": "iRVZG_hKttdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al parecer se trata de un atributo con valores 0's y 1's, donde 0 corresponder칤a a que el paciente no presenta enfermedad cardiovascular, mientras que 1 significa que si tiene. Para comprobar esto, utilizamos la funci칩n `set` que nos permite ver todos los valores distintos que puede tomar este atributo."
      ],
      "metadata": {
        "id": "_UCz9MKqSDUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(df[\"cardiovascular_disease\"])"
      ],
      "metadata": {
        "id": "6jUTDrTMuCwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con este resultados podemos concluir que trabajaremos con una clasificaci칩n binaria 游. Otro paso fundamental, es contar la frecuencia de cada uno de los posibles valores pertenecientes a nuestro atributo/variable objetivo. Esto se realiza con la funci칩n `value_counts`."
      ],
      "metadata": {
        "id": "fFAkiosDSJ_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"cardiovascular_disease\"].value_counts()"
      ],
      "metadata": {
        "id": "PVaXCrXLt3jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que los datos est치n pr치cticamente balanceados; 164 pacientes presentan una enfermedad cardiovascular, mientras que 137 no tienen. En caso que exista un desbalance, hay que aplicar t칠cnicas especiales de balanceo que veremos m치s adelante en el curso. Ahora sigamos analizando nuestros datos viendo un ejemplo."
      ],
      "metadata": {
        "id": "cgze1JsWuWkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(1)"
      ],
      "metadata": {
        "id": "uiOYyt7GuwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que las columnas *chest_pain_type* y *thalassemia* son atributos con valores de tipo string (palabras). En aprendizaje de m치quinas nuestros algoritmos o sistemas est치n basados en una serie de operaciones matem치ticas sobre los datos, lo cual nos permite entrenar y luego realizar predicciones. Debido a esto, los modelos no pueden trabajar directamente con texto y estos deben ser transformados a n칰meros.\n",
        "\n",
        "La manera m치s sencilla de realizar esto es utilizando una transformaci칩n **One-Hot**. Esto significa que si tenemos un atributo de tipo string, el cual tiene `n` valores distintos, crearemos un total de `n` columnas nuevas con valores num칠ricos y binarios. Para ejemplificar, si se tiene una columna con los siguientes valores\n",
        "\n",
        "\n",
        "| chest_pain_type |\n",
        "|--------|\n",
        "| asymptomatic |\n",
        "| non_anginal_pain   |\n",
        "| typical_angina|\n",
        "| atypical_angina |\n",
        "\n",
        "y se transforma utilizando One-Hot Encoding, se generan las siguientes asignaciones columnas:\n",
        "\n",
        "\n",
        "| chest_pain_type_asymptomatic | chest_pain_type_non_anginal_pain | chest_pain_type_typical_angina | chest_pain_type_atypical_angina |\n",
        "|------------------------------|----------------------------------|--------------------------------|---------------------------------|\n",
        "| 1                            | 0                                | 0                              | 0                               |\n",
        "| 0                            | 1                                | 0                              | 0                               |\n",
        "| 0                            | 0                                | 1                              | 0                               |\n",
        "| 0                            | 0                                | 0                              | 1                               |\n",
        "\n",
        "\n",
        "As칤, ya no tenemos que lidiar con valores no num칠ricos, y la cantidad de caracter칤sticas aumenta. Para realizar esta transformaci칩n podemos utilizar la funci칩n `get_dummies`."
      ],
      "metadata": {
        "id": "g5wLI595vM9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df)\n",
        "df.sample(3)"
      ],
      "metadata": {
        "id": "x1pkmruDwTgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un dato que puede ser 칰til a la hora de analizar datasets, es que los dataframes de pandas nos permiten realizar consultas sobre nuestro dataset con los filtros. Por ejemplo, con el siguiente comando consultamos cu치ntas personas menores a 40 a침os presentan una enfermedad cardiovascular."
      ],
      "metadata": {
        "id": "XL8YdWb8T_-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df[\"age\"] <= 40) & (df[\"cardiovascular_disease\"] == 1)].shape[0]"
      ],
      "metadata": {
        "id": "Mr7bGlZYTjc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo siguiente que podemos hacer, es crear diferentes visualizaciones que nos permitan entender mejor nuestros datos. \n",
        "\n",
        "Por ejemplo, si utilizamos la funci칩n *box plot* podemos visualizar si es que existen datos alejados del promedio o con un extra침o comportamiento en nuestro dataset, m치s conocidos como outliers."
      ],
      "metadata": {
        "id": "BEw9DVw-Uk4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['age', 'cholesterol','resting_blood_pressure']].plot.box(vert = False, grid = True)"
      ],
      "metadata": {
        "id": "I7vw6acuUr6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi칠n podemos utilizar un scatter plot para ver si dos variables tienen una relaci칩n lineal o alg칰n otro comportamiento"
      ],
      "metadata": {
        "id": "OC6UjVaYVlVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df['age'], df['cardiovascular_disease'])\n",
        "plt.xlabel('Edad')\n",
        "plt.ylabel('Enfermedad Cardiovascular')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pFzY5_CTVpdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la funci칩n `corr` visualizamos la matriz de correlaci칩n entre nuestros atributos"
      ],
      "metadata": {
        "id": "x6fgPym9WIZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df.corr()\n",
        "features = corr.index\n",
        "plt.figure(figsize=(15,15))\n",
        "ax = sns.heatmap(df[features].corr(), annot=True)"
      ],
      "metadata": {
        "id": "njUJvBeCWGwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento de modelos**"
      ],
      "metadata": {
        "id": "266x1MIjzglM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder entrenar nuestros modelos de aprendizaje supervisado debemos separar nuestros datos identificando claramente cu치l ser치 nuestra variable objetivo, es decir, la que queremos predecir. \n",
        "\n",
        "Generalmente se utiliza la variable `X` para los atributos de entrada o features, mientras que la variable `y` es el objetivo."
      ],
      "metadata": {
        "id": "LZ51pRHeXYk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"cardiovascular_disease\", axis=1) # Drop elimina la columna que se especifique, y axis = 1 significa eliminar valores de las columnas"
      ],
      "metadata": {
        "id": "cqOtWGnCXiBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['cardiovascular_disease']"
      ],
      "metadata": {
        "id": "DqpOMUOyXnW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teniendo ambos vectores, ya podemos entrenar modelos sobre nuestros datos 游땏. En particular, veremos 3 modelos: Support Vector Machines (SVM), Decision Tree Classifier (DTC), y Randon Forest (RF).\n"
      ],
      "metadata": {
        "id": "nc74_GWvX1X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Classification (SVC)**\n",
        "\n"
      ],
      "metadata": {
        "id": "bQiafoy-cwg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una explicaci칩n muy general sobre este modelo, es que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo m치s amplios posibles mediante un hiperplano de separaci칩n. Cuando llega un nuevo ejemplo, se analiza a cu치l de los dos espacios pertenece para realizar la clasificaci칩n. En general, este algoritmo funciona muy bien con datos que son linealmente separables. En caso que no sea as칤, este algoritmo puede trasladar el problema de un problema no separable linealmente a uno que s칤 lo es. Veamos como podemos programar un modelo SVC."
      ],
      "metadata": {
        "id": "yvJfSL9bHqBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "nOmOjFUcc3i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_model = SVC() # Creamos un objeto de tipo SVC.\n",
        "svc_model.fit(X, y) # Entrenamos el modelo con nuestros vectores X e y"
      ],
      "metadata": {
        "id": "KgQhQhD5X8wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de que el modelo fue entrenado, podemos hacer  predicciones sobre alg칰n conjunto de datos para as칤 analizar el rendimiento de nuestro clasificador. Este proceso se realiza mediante la funci칩n `predict`."
      ],
      "metadata": {
        "id": "llWRVYegvdxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = svc_model.predict(X)"
      ],
      "metadata": {
        "id": "87GyzdhGXyqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evaluar el rendimiento de nuestro modelo utilizamos la funci칩n `accuracy_score`. Esta funci칩n nos permite obtener la proporci칩n de ejemplos que fueron correctamente clasificados sobre el total."
      ],
      "metadata": {
        "id": "v9lYyPT4v6L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vPhWTKS5c9k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'El accuracy obtenido por el modelo SVC es: {np.round(accuracy_score(y, predictions), 2)}')"
      ],
      "metadata": {
        "id": "bCtQlYZJYU4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y listo, hemos entrenado nuestro clasificador y hemos obtenido resultados bastante altos dada la complejidad del problema 游봅 \n",
        "\n"
      ],
      "metadata": {
        "id": "FwXv_i_GYYRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fired!](https://media3.giphy.com/media/3EAYL7KCtZJOJGtli6/giphy.gif?cid=ecf05e47b8ucx71fgr2bny9bn1mf8osjbh2z2grz4c4v4pyd&rid=giphy.gif&ct=g)"
      ],
      "metadata": {
        "id": "dzfnsSTNZ1pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin embargo, hemos entrenamos el modelo sobre el conjunto de datos `X` e `y` y luego medimos el rendimiento sobre el mismo conjunto `y` original, es decir, no estamos midiendo correctamente el rendimiento del modelo en datos nuevos. Es como si llegaran 10 pacientes a la consulta, aprendemos cu치l de ellos tiene o no tiene una enfermedad vascular. Y luego, evaluamos que tan bien clasificamos a los mismos 10 pacientes si tienen o no tienen la enfermedad. Esto no s칩lo es hacer trampa, sino que nos lleva al problema de **generalizaci칩n** en aprendizaje de m치quinas.\n",
        "\n",
        "쯈u칠 es la generalizaci칩n?\n",
        "\n",
        "Supongamos que ustedes son unos estudiantes de primer a침o de universidad. Para poder estudiar para su primer control recolectan los controles realizados en toda la historia del ramo, y con una gran memoria se aprenden de memoria todas las preguntas y respuestas. \n",
        "\n",
        "Si al momento de rendir la prueba, aparece alguna de esas mismas preguntas, ustedes obtendr치n una buena nota ya que saben la respuesta. Sin embargo, si por alguna raz칩n el profesor decide innovar y crea una nueva pregunta, la nota que obtendr치n probablemente ser치 mala, ya que no estaban preparados para una nueva pregunta, un nuevo dato. Eso es lo que ocurre ac치, no sabemos si el modelo es capaz de obtener buenos resultados en nuevos datos. \n",
        "\n",
        "쮺칩mo podemos solucionar esto?\n",
        "\n",
        "Es importante antes de entrenar un modelo dividir nuestros datos en dos conjuntos; entrenamiento y testeo. El primer conjunto nos servir치 para entrenar nuestro modelo, mientras que el segundo nos servir치 para medir el rendimiento en datos que con alta probabilidad no fueron vistos anteriormente. Esto nos permite medir el nivel de **generalizaci칩n** de los modelos.\n",
        "\n",
        "M치s adelante veremos que en la pr치ctica es mejor dividir el conjunto original en 3 particiones; entrenamiento, validaci칩n y testeo. Pero por ahora, nos quedaremos con las dos mencionadas. Para generar estos conjuntos lo hacemos con el siguiente c칩digo:"
      ],
      "metadata": {
        "id": "Evcn7evTYn93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "xI4PwBWAfhRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
      ],
      "metadata": {
        "id": "PF-Smf6SYhS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego, creamos y entrenamos nuestro modelo sobre el conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "N6vKcUJedB2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc_model = SVC()\n",
        "svc_model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "WNFvBMfAZ938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos las predicciones sobre el conjunto de test y calculamos el rendimiento."
      ],
      "metadata": {
        "id": "Ibt3z0CYdGy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = svc_model.predict(X_test)"
      ],
      "metadata": {
        "id": "qBJmaHtGaATO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'El accuracy obtenido por el modelo SVC es: {np.round(accuracy_score(Y_test, predictions), 2)}')"
      ],
      "metadata": {
        "id": "amDYkqOJaHRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos obtener los indices de los datos que nos permiten crear el hiperplano\n",
        "support_vector_indices = svc_model.support_\n",
        "print(support_vector_indices)"
      ],
      "metadata": {
        "id": "x9zgzGtrKmrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que estamos trabajando el m칰ltiples dimensiones no es posible generar un gr치fico adecuado, sin embargo, si tuvieramos un problema de clasificaci칩n binaria con s칩lo dos atributos de entrada, podr칤amos generar visualizaciones como la siguiente. Este m칠todo es diferente de una Regresi칩n logistica, ya que busca encontrar la m치xima separaci칩n entre los conjuntos, no una s칩la recta."
      ],
      "metadata": {
        "id": "mWjiUgRvLkuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image info](https://github.com/christianversloot/machine-learning-articles/raw/main/images/support_vectors.png)"
      ],
      "metadata": {
        "id": "AP8Wuq45L2f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decission Tree Classifier**"
      ],
      "metadata": {
        "id": "sYrcSDBGwgAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M칠todo de aprendizaje supervisado que permite resolver tareas de regresi칩n y clasificaci칩n. Se llama 치rbol ya que visualmente parece un 치rbol invertido, y est치 compuesto tanto por una ra칤z, ramas y hojas como veremos m치s adelante. "
      ],
      "metadata": {
        "id": "ZNGIGR0gohOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree"
      ],
      "metadata": {
        "id": "0M_ZfQQIwwmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = DecisionTreeClassifier( # Instanciamos nuestro 치rbol de decisi칩n.\n",
        "    max_depth=3, # Forzamos que nuestro 치rbol s칩lo tenga 3 niveles de profundidad.\n",
        "    random_state = 11\n",
        "    )\n",
        "tree.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "dXfDusyuwisx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_predictions = tree.predict(X_test)"
      ],
      "metadata": {
        "id": "KEWozp9Qw34m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'El accuracy obtenido por el modelo Decision Tree es: {np.round(accuracy_score(Y_test, tree_predictions), 2)}')"
      ],
      "metadata": {
        "id": "WzcvXmaaw9Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las ventajas m치s importantes de los 치rboles de decisi칩n es la transparencia y explicabilidad del proceso de predicci칩n. Podemos conocer claramente todas las decisiones tomadas por el clasificador y visualizarla en un 치rbol de decisi칩n."
      ],
      "metadata": {
        "id": "sgy-CBPZxM5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "plot_tree( # Funci칩n que nos permite visualizar el 치rbol de decisi칩n ajustado.\n",
        "    tree,\n",
        "    class_names = [\"healthy\",\"sick\"],\n",
        "    feature_names = df.columns,\n",
        "    filled=True,\n",
        "    fontsize=11,\n",
        "    proportion = True,\n",
        "    label = \"all\"\n",
        "    \n",
        "    \n",
        "    )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WERHy5XxxIFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se ve en la figura anterior, este m칠todo evalua todas las variables de entrada para seleccionar la mejor salida. La variable m치s significativa se encontrar치 en la base del 치rbol creando una condici칩n, a partir de esto se generan n posibilidades dependiendo la cantidad de posibles valores de la variable objetivo, luego se pasa al siguiente nodo y se repite el proceso, hasta llegar a una hoja."
      ],
      "metadata": {
        "id": "USveMliqpkfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evaluar el nivel de importancia que tienen las caracter침isticas, los algoritmos basados en 치rboles nos ofrecen la importancia de cada variable al calcular normalmente el Mean Decrease Gini, pueden encontrar m치s informaci칩n ac치 https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3"
      ],
      "metadata": {
        "id": "WgBaT-m9x4aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_vil = pd.DataFrame(list(zip(df.columns,tree.feature_importances_)),\n",
        "             columns=[\"feature\",\"importance\"]\n",
        "            ).set_index(\"feature\")\n",
        "tree_vil.sort_values(\"importance\",ascending=False)"
      ],
      "metadata": {
        "id": "0ET4f-1kxeqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uno de los hiperpar치metros de los 치rboles de decisi칩n es la profundidad del mismo. Evaluemos c칩mo se comporta su rendimiento al cambiar la profundidad del 치rbol."
      ],
      "metadata": {
        "id": "vRIL8ap8yOgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depths = range(1,10)\n",
        "performances = []\n",
        "for depth in depths:\n",
        "    current_tree = DecisionTreeClassifier( # Instanciamos nuestro 치rbol de decisi칩n.\n",
        "    max_depth=depth, # Forzamos que nuestro 치rbol s칩lo tenga 3 niveles de profundidad.\n",
        "    random_state = 11\n",
        "    )\n",
        "    current_tree.fit(X_train, Y_train)\n",
        "    predictions = current_tree.predict(X_test)\n",
        "\n",
        "    performances.append(accuracy_score(Y_test, predictions))"
      ],
      "metadata": {
        "id": "Jy5tAp5JyDpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(\n",
        "    depths,\n",
        "    performances\n",
        ")\n",
        "plt.xlabel(\"max_width\")\n",
        "plt.ylabel(\"mean_roc_auc_score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "64IzN_9xyWYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Classifier**"
      ],
      "metadata": {
        "id": "2Rid_vFdzYwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "M칠todo basado en el modelo anterior. La idea es que en vez de tener un 칰nico 치rbol, existir치n m칰ltiples, de ah칤 viene el t칠rmino forest. Dado que cada 치rbol entrega una clasificaci칩n, la clasificaci칩n quedar치 determinada por el valor que contenga m치s votos. Entre m치s 치rboles haya, m치s robusto y preciso ser치 el algoritmo."
      ],
      "metadata": {
        "id": "hz1XcH_QptYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "Avh9DrwczgD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "PDZBOQaxzald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_predictions = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "Nn7gsg3UzmBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(Y_test, rf_predictions)"
      ],
      "metadata": {
        "id": "aKPBXoU0zmUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_vil = pd.DataFrame(list(zip(df.columns,rf.feature_importances_)),\n",
        "             columns=[\"feature\",\"importance\"]\n",
        "            ).set_index(\"feature\")\n",
        "rf_vil.sort_values(\"importance\",ascending=False)"
      ],
      "metadata": {
        "id": "zb7-BlyFzsf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_tree = rf.estimators_[-3]"
      ],
      "metadata": {
        "id": "CE_h7qj8zwde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "plot_tree( # Funci칩n que nos permite visualizar el 치rbol de decisi칩n ajustado.\n",
        "    random_tree, # Objeto de nuestro 치rbol de decisi칩n entrenado.\n",
        "    feature_names = df.columns, # Nombres de las variables utilizadas para entrenar.\n",
        "    class_names = [\"healthy\",\"sick\"], # Nombre de las clases que estamos prediciendo.\n",
        "    label = \"all\", # Etiquetamos todas caracter칤sticas de cada nodo.\n",
        "    proportion = True, # Visualizamos las proporciones de datos en cada nodo de decisi칩n,\n",
        "    filled=True, # Coloreamos los nodos\n",
        "    fontsize=11, # Establecemos el tama침o de la letra del texto dentro de cada nodo.,\n",
        "    max_depth=3 # Profundidad m치xima del 치rbol\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kq3i1KdVzzJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}